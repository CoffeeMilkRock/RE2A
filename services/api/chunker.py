from __future__ import annotations
from typing import Any, Dict, List
from langchain.schema import Document
import re
from langchain.text_splitter import RecursiveCharacterTextSplitter
import json
# C√°c key KH√îNG ƒë∆∞a v√†o text (ƒëi kh·∫Øp JSON theo path)
NOISE_KEYS = {
    "misc.created_at", "misc.updated_at",
    "design_and_layout.id", "design_and_layout.code", "design_and_layout.uuid",
    "physical_features.id", "physical_features.code",
    "legal_and_product_status.id", "legal_and_product_status.code",
    "equipment_and_handover_materials.id",
    # M·∫´u t·ªïng qu√°t ·ªü d∆∞·ªõi x·ª≠ l√Ω th√™m: .*id, .*uuid, .*code
}

# Allowlist theo section (n·∫øu section c√≥ trong ALLOW_SECTIONS th√¨ CH·ªà gi·ªØ c√°c key c√≥ prefix n√†y)
ALLOW_SECTIONS = {
    # Ch·ªâ l√† v√≠ d·ª•‚Äît√πy d·ªØ li·ªáu th·ª±c t·∫ø c·ªßa b·∫°n
    "design_and_layout": {
        "design_and_layout.location",
        "design_and_layout.type",
        "design_and_layout.property_type",
        "design_and_layout.bedrooms",
        "design_and_layout.bathrooms",
        "design_and_layout.area",
        "design_and_layout.price",
        "design_and_layout.floor",
    },
    "living_experience": set(),  # r·ªóng = cho ph√©p t·∫•t c·∫£ (tr·ª´ noise)
    "physical_features": set(),
    "legal_and_product_status": {
        "legal_and_product_status.status",
        "legal_and_product_status.red_book",
        "legal_and_product_status.ownership",
    },
}

# Heuristic l·ªçc gi√° tr·ªã
MIN_VALUE_CHARS = 4           # b·ªè c√°c m·∫©u qu√° ng·∫Øn (vd: "ok", "N/A")
MAX_VALUE_CHARS = 600         # c·∫Øt b·ªõt gi√° tr·ªã qu√° d√†i  (per value)
MAX_LIST_ITEMS_PER_SECTION = 50  # list qu√° d√†i th√¨ c·∫Øt b·ªõt

# Regex ƒë·ªÉ b·ªè chu·ªói c√≥ √≠t ng·ªØ nghƒ©a
RE_MOSTLY_NUMERIC = re.compile(r"^[\d\W_]+$")  # to√†n s·ªë/k√Ω t·ª± kh√¥ng ch·ªØ
RE_UUID_LIKE = re.compile(r"\b[0-9a-f]{8}-[0-9a-f]{4}-[1-5][0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}\b", re.I)
RE_URL = re.compile(r"https?://\S+")
RE_CODEY = re.compile(r"[{}[\];<>]{3,}")
RE_KEY_NOISE_SUFFIX = re.compile(r"(?:^|\.)(?:id|uuid|code|created_at|updated_at)(?:\[.*\])?$", re.I)

def _flatten(obj: Any, prefix: str = "", section: str | None = None) -> List[tuple[str, str]]:
    """
    √âp JSON th√†nh list (key_path, text_value) v·ªõi l·ªçc noise theo key/value.
    """
    out: List[tuple[str, str]] = []

    def to_str(x: Any) -> str:
        if x is None:
            return ""
        if isinstance(x, (str, int, float, bool)):
            return str(x)
        # stringify g·ªçn cho object ph·ª©c t·∫°p
        try:
            s = json.dumps(x, ensure_ascii=False)
            return s
        except Exception:
            return str(x)

    def rec(node: Any, path: str, lvl_section: str | None):
        # L·ªçc theo key ngay t·∫°i ƒë√¢y: n·∫øu path tr√∫ng noise key th√¨ b·ªè
        if path and _is_noise_key(path, lvl_section):
            return
        
        if isinstance(node, dict):
            for k, v in node.items():
                child_path = f"{path}.{k}" if path else k
                rec(v, child_path, lvl_section)
        elif isinstance(node, list):
            # c·∫Øt b·ªõt list d√†i
            for i, v in enumerate(node[:MAX_LIST_ITEMS_PER_SECTION]):
                child_path = f"{path}[{i}]"
                rec(v, child_path, lvl_section)
        else:
            text = to_str(node).strip()
            # L·ªçc theo gi√° tr·ªã
            if not text or _is_noise_value(text):
                return
            text = _truncate_value(text)
            out.append((path, text))

    rec(obj, prefix, section)
    return out

def _safe_float(v: Any) -> float | None:
    try:
        if v is None: return None
        if isinstance(v, (int, float)): return float(v)
        s = str(v).replace(",", "").strip()
        return float(s) if s else None
    except: return None

def _first_non_empty(data: dict, keys: List[str]) -> str | None:
    def get_path(d, path: str):
        cur = d
        for part in path.split("."):
            if "[" in part and part.endswith("]"):
                name = part[:part.index("[")]
                idx = int(part[part.index("[")+1:-1])
                cur = (cur.get(name) or [])[idx] if isinstance(cur, dict) else None
            else:
                cur = cur.get(part) if isinstance(cur, dict) else None
            if cur is None: return None
        return cur
    for k in keys:
        val = get_path(data, k)
        if val not in (None, "", []): return str(val)
    return None

def json_to_documents(property_json: Dict[str, Any]) -> List[Document]:
    pid = property_json.get("id") or property_json.get("unitId") or "UNKNOWN"
    buckets = {
        "description": property_json.get("description", ""),
        "design_and_layout": property_json.get("design_and_layout", {}),
        "living_experience": property_json.get("living_experience", {}),
        "physical_features": property_json.get("physical_features", {}),
        "equipment_and_handover_materials": property_json.get("equipment_and_handover_materials", {}),
        "legal_and_product_status": property_json.get("legal_and_product_status", {}),
        "property_groups": property_json.get("property_groups", []),
        "misc": {
            "unitId": property_json.get("unitId"),
            "created_at": property_json.get("created_at"),
            "updated_at": property_json.get("updated_at"),
        },
    }

    docs: List[Document] = []
    for section, content in buckets.items():
        parts = _flatten(content, prefix=section, section=section)  # <-- truy·ªÅn section
        if not parts:
            continue

        # Gh√©p c√°c c·∫∑p (key_path, value) th√†nh text ‚Äús·∫°ch‚Äù h∆°n
        # B·∫°n c√≥ th·ªÉ b·ªè prefix [key] n·∫øu mu·ªën, m√¨nh gi·ªØ l·∫°i ƒë·ªÉ truy v·∫øt
        lines = [f"[{k}] {v}" for k, v in parts]
        text_block = "\n".join(lines)

        metadata = {
            "property_id": pid,
            "section": section,
            "unitId": property_json.get("unitId"),
            "location": _first_non_empty(property_json, ["design_and_layout.location","physical_features.location"]),
            "property_type": _first_non_empty(property_json, ["design_and_layout.type","design_and_layout.property_type"]),
            "bedrooms": _safe_float(_first_non_empty(property_json, ["design_and_layout.bedrooms"])),
            "price": _safe_float(_first_non_empty(property_json, ["design_and_layout.price"])),
        }

        # üîß 1) √©p ki·ªÉu v·ªÅ primitive v√† b·ªè None
        cleaned = {}
        for k, v in metadata.items():
            if v is None:
                continue
            # chu·∫©n ho√° string
            if k in ("location", "property_type", "unitId", "section", "property_id"):
                cleaned[k] = str(v)
            # numeric gi·ªØ float
            elif k in ("bedrooms", "price"):
                try:
                    cleaned[k] = float(v)
                except Exception:
                    continue
            else:
                # fallback: stringify
                cleaned[k] = str(v)

        metadata = cleaned
        docs.append(Document(page_content=text_block, metadata=metadata))   
    return docs
def chunk_documents(
    docs: List[Document],
    chunk_size: int = 800,
    chunk_overlap: int = 120
) -> List[Document]:
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap,
        add_start_index=True,
        separators=["\n\n","\n",". "," ",""],
    )
    out: List[Document] = []
    for d in docs:
        chs = splitter.split_documents([d])
        for i, c in enumerate(chs):
            c.metadata = dict(c.metadata or {})
            c.metadata["chunk_index"] = i
        out.extend(chs)
    return out

def _is_noise_key(path: str, section: str | None = None) -> bool:
    # B·ªè n·∫øu trong NOISE_KEYS ho·∫∑c c√≥ h·∫≠u t·ªë "id/uuid/code/created_at/updated_at"
    if path in NOISE_KEYS:
        return True
    if RE_KEY_NOISE_SUFFIX.search(path):
        return True

    # N·∫øu section c√≥ allowlist: ch·ªâ gi·ªØ key b·∫Øt ƒë·∫ßu ƒë√∫ng allow
    if section and section in ALLOW_SECTIONS and ALLOW_SECTIONS[section]:
        allowed = ALLOW_SECTIONS[section]
        # Cho qua n·∫øu path b·∫Øt ƒë·∫ßu b·∫±ng b·∫•t k·ª≥ prefix trong allowed
        return not any(path.startswith(p) for p in allowed)

    return False

def _is_noise_value(text: str) -> bool:
    if not text or len(text) < MIN_VALUE_CHARS:
        return True
    if RE_MOSTLY_NUMERIC.match(text):
        return True
    if RE_UUID_LIKE.search(text):
        return True
    if RE_URL.search(text):
        # URL th∆∞·ªùng kh√¥ng h·ªØu √≠ch cho semsearch (tr·ª´ khi b·∫°n mu·ªën gi·ªØ)
        return True
    if RE_CODEY.search(text):
        return True
    return False

def _truncate_value(text: str) -> str:
    if not text:
        return text
    if len(text) > MAX_VALUE_CHARS:
        return text[:MAX_VALUE_CHARS] + "‚Ä¶"
    return text